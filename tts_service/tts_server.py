import os
import io
import numpy as np
import torch
from flask import Flask, request, send_file

app = Flask(__name__)

# === –ù–ê–°–¢–†–û–ô–ö–ê TORCH / –ú–û–î–ï–õ–ò ===
device = torch.device("cpu")
torch.set_num_threads(4)

MODEL_PATH = "v5_ru.pt"

# –°–∫–∞—á–∏–≤–∞–µ–º –∏–º–µ–Ω–Ω–æ v5_ru.pt, –º–∏–Ω—É—è hub-–∫—ç—à
if not os.path.isfile(MODEL_PATH):
    torch.hub.download_url_to_file(
        "https://models.silero.ai/models/tts/ru/v5_ru.pt",
        MODEL_PATH
    )

print("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑", MODEL_PATH)
model = torch.package.PackageImporter(MODEL_PATH).load_pickle("tts_models", "model")
model.to(device)

speakers = getattr(model, "speakers", [])
print("‚úÖ –°–ø–∏–∫–µ—Ä—ã –º–æ–¥–µ–ª–∏:", speakers)

# ---- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ¬´–≤–µ—Ä—Å–∏–∏¬ª –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º ----
model_type = type(model).__name__
apply_args = model.apply_tts.__code__.co_varnames

if MODEL_PATH == "v5_ru.pt" and set(speakers) == {"aidar", "baya", "kseniya", "eugene", "xenia"}:
    detected_version = "v5_ru (core v3 + v5 —Ñ–∏—á–∏)"
elif "v3" in model_type.lower():
    detected_version = "v3 (–ø–æ –∏–º–µ–Ω–∏ –∫–ª–∞—Å—Å–∞)"
elif "v5" in model_type.lower():
    detected_version = "v5 (–ø–æ –∏–º–µ–Ω–∏ –∫–ª–∞—Å—Å–∞)"
elif "use_stress" in apply_args or "use_yo" in apply_args or "put_stress_homo" in apply_args:
    detected_version = "v5-–ø–æ–¥–æ–±–Ω–∞—è (–µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–¥–∞—Ä–µ–Ω–∏–π/–æ–º–æ–≥—Ä–∞—Ñ–æ–≤)"
elif "aidar" in speakers and "baya" in speakers and "random" in speakers:
    detected_version = "v4_ru-–ø–æ–¥–æ–±–Ω–∞—è (speakers + random)"
else:
    detected_version = "–Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ (–Ω—É–∂–Ω–∞ —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)"

print("üîé –¢–∏–ø –∫–ª–∞—Å—Å–∞ –º–æ–¥–µ–ª–∏:", model_type)
print("üîé –ü–∞—Ä–∞–º–µ—Ç—Ä—ã apply_tts:", apply_args)
print("üîé –ì—Ä—É–±–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏:", detected_version)
# -------------------------------------------


def preprocess_text(text: str) -> str:
    """–ü—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—É–∑ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ—Å–æ–¥–∏–∏."""
    text = (text or "").strip()
    if not text:
        return text

    # –ó–∞–º–µ–Ω–∞ ... –Ω–∞ –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª (—á–∞—Å—Ç–æ –ª—É—á—à–µ –¥–ª—è TTS)
    text = text.replace("...", "‚Ä¶")

    # –ü–∞—É–∑—ã –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –∫–æ–Ω—Ü–∞ —Ñ—Ä–∞–∑—ã
    for ch in [".", "!", "?"]:
        text = text.replace(ch, ch + " ")

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = " ".join(text.split())
    return text


def time_stretch_tensor(audio: torch.Tensor, speed_factor: float) -> torch.Tensor:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∏–π time-stretch —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—É—é –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é.
    speed_factor < 1 => –º–µ–¥–ª–µ–Ω–Ω–µ–µ, > 1 => –±—ã—Å—Ç—Ä–µ–µ.
    –ú–µ–Ω—è–µ—Ç—Å—è –∏ —Å–∫–æ—Ä–æ—Å—Ç—å, –∏ –≤—ã—Å–æ—Ç–∞ –≥–æ–ª–æ—Å–∞ –Ω–µ–º–Ω–æ–≥–æ, –Ω–æ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫.
    """
    if speed_factor <= 0:
        return audio

    if speed_factor == 1.0:
        return audio

    y = audio.numpy()
    n_src = len(y)
    n_tgt = int(n_src / speed_factor)

    if n_tgt < 2 or n_src < 2:
        return audio

    # –Ω–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    src_positions = np.linspace(0, n_src - 1, num=n_tgt)
    src_indices = np.floor(src_positions).astype(int)
    src_indices_next = np.clip(src_indices + 1, 0, n_src - 1)
    frac = src_positions - src_indices

    y_stretched = (1.0 - frac) * y[src_indices] + frac * y[src_indices_next]
    return torch.from_numpy(y_stretched.astype(np.float32))


@app.route("/synthesize", methods=["POST"])
def synthesize():
    data = request.get_json(force=True)
    text = preprocess_text(data.get("text", ""))

    if not text:
        return {"error": "–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π"}, 400

    # –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º —Å–¥–µ–ª–∞—Ç—å –≤—ã–±–æ—Ä —Å–ø–∏–∫–µ—Ä–∞ –∏–∑ JSON
    speaker = "xenia"
    print(f"üìù –ó–∞–ø—Ä–æ—Å: '{text[:60]}...' | üé§ speaker={speaker} | model={detected_version}")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –≤–∫–ª—é—á—ë–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏ v5 (—É–¥–∞—Ä–µ–Ω–∏—è / –æ–º–æ–≥—Ä–∞—Ñ—ã / —ë)
    audio = model.apply_tts(
        text=text,
        ssml_text=None,
        speaker=speaker,
        sample_rate=48000,
        put_accent=True,
        put_stress_homo=True,
        put_yo=True,
        put_yo_homo=True,
        stress_single_vowel=False,
        voice_path=None,
        symbol_durs=None,
        return_ts=False,
    )

    # === –ó–ê–ú–ï–î–õ–ï–ù–ò–ï –†–ï–ß–ò ===
    # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏: 0.8 -> –º–µ–¥–ª–µ–Ω–Ω–µ–µ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 20%
    # speed_factor = float(data.get("speed", 0.9))
    # audio = time_stretch_tensor(audio, speed_factor)

    # –õ—ë–≥–∫–∞—è –ø–æ—Å—Ç‚Äë–æ–±—Ä–∞–±–æ—Ç–∫–∞: headroom + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + fade-in/out
    audio = torch.clamp(audio, -0.95, 0.95)
    audio = torch.nn.functional.normalize(audio, dim=0)

    fade_samples = int(0.03 * 48000)  # 30 ms
    if len(audio) > fade_samples * 2:
        fade = torch.linspace(0, 1, fade_samples)
        audio[:fade_samples] *= fade
        audio[-fade_samples:] *= torch.linspace(1, 0, fade_samples)

    # float32 [-1,1] -> int16 stereo BIG-ENDIAN (–¥–ª—è Discord/JDA)
    mono = (audio.numpy() * 32767).astype(np.int16)
    stereo = np.stack([mono, mono], axis=1)
    stereo_be = stereo.astype(">i2")
    pcm_bytes = stereo_be.tobytes()

    # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ–¥ 20 ms –ø–∞–∫–µ—Ç—ã (3840 –±–∞–π—Ç)
    packet_size = 3840
    aligned_len = (len(pcm_bytes) // packet_size) * packet_size
    pcm_bytes = pcm_bytes[:aligned_len]

    buf = io.BytesIO(pcm_bytes)
    buf.seek(0)
    return send_file(
        buf,
        mimetype="application/octet-stream",
        download_name="speech.pcm"
    )


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
